{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KirvBkEGvaZd"
      },
      "source": [
        "# Large assignment 2\n",
        "\n",
        "## Read before you start\n",
        "\n",
        "* Provide clear and complete answers in code blocks or markdown. You may add as many as you need.\n",
        "* Always motivate your answers. This can be done in markdown cells, or in comments in code.\n",
        "* Submit your results via Brightspace. Use the following filename convention: ``StudentName1_snumber1_StudentName2_snumber2_LargeAssignment2.ipynb``.\n",
        "* Make sure you submit a fully executed version of the notebook file. The teaching assistants will not run/debug your code during grading.\n",
        "* Questions? Ask them during the workgroups, or see Brightspace for instructions on how to proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiVW3AShvgQl",
        "outputId": "fbf27843-3a6d-4c67-d23e-eeb62173bd87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyJAGS v1.3.8\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pyjags as pj\n",
        "\n",
        "print('Using PyJAGS v{:s}'.format(pj.__version__))\n",
        "\n",
        "plt.rc('axes', titlesize=18)        # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=18)        # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=14)       # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=14)       # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=12)       # legend fontsize\n",
        "plt.rc('figure', titlesize=30)      # fontsize of the figure title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MXSXSCwwvAe"
      },
      "source": [
        "# 1. Dentistry\n",
        "\n",
        "## 1.1 The pilot study\n",
        "\n",
        "The company DentoCare has developed a new toothbrush, the 'Dentinator 2000'. The toothbrush is first being introduced in France. After DentoCare has introduced the toothbrush to the market, they polled a number of French dentists and asked whether they prefer the Dentinator 2000 or its predecessor, the 'Dentinator Pro'.\n",
        "\n",
        "The results from the poll are that out of $N_F=238$ French dentists, $z_F=187$ say they recommend the new toothbrush. \n",
        "\n",
        "DentoCare assumes that, if the Dentinator is performing equal to its predecessor, the recommendations would be 50/50 in favor or against the new toothbrush. To test whether the dentists really prefer the new toothbrush, DentoCare performs Bayesian model comparison.\n",
        "\n",
        "They define two models, $m_1$, in which there is a preference, and $m_0$ in which the recommendations are pretty much 50/50. In both models, the company uses a binomial likelihood:\n",
        "\n",
        "$$\n",
        "    p(z=k \\mid N, \\theta) = \\binom{N}{z} \\theta^z (1-\\theta)^{N-z} \\enspace.\n",
        "$$\n",
        "\n",
        "For the null model, DentoCare uses a spike distribution with $c=0.5$:\n",
        "\n",
        "$$\n",
        "    p(\\theta \\mid c) = \\text{spike}(\\theta \\mid c) = \\begin{cases}1 & \\text{if $\\theta=c$,} \\\\ 0 & \\text{otherwise}\\end{cases} \\enspace,\n",
        "$$ as the prior.\n",
        "\n",
        "For the alternative model, they use our familiar beta distribution as a prior instead:\n",
        "\n",
        "$$\n",
        "    p(\\theta \\mid a, b) = \\frac{1}{B(a,b)} \\theta^{a-1} (1-\\theta)^{b-1} \\enspace.\n",
        "$$\n",
        "\n",
        "In this setting, the Bayes factor $BF_{10}$ can be computed exactly, because of the chosen priors. To compute $BF_{10}$, we need the marginal likelihoods of both models.\n",
        "\n",
        "1. Derive the expression for the marginal likelihood of $m_1$, $p(z \\mid N, m_1)$, in terms of $a, b, z, N$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Tr3TmR2Rg2"
      },
      "source": [
        "_ANSWER:_ \n",
        "$$\n",
        "\\begin{align}\n",
        "\n",
        "\n",
        "p(z|N,m_{1})&= \\int_{0}^{1}  p(z|N,\\Theta) p(\\Theta|a,b) \\ d\\Theta \\\\\n",
        "&= \\int_{0}^{1} \\binom{N}{z} \\theta^z (1-\\theta)^{N-z} \\enspace\\frac{1}{B(a,b)} \\theta^{a-1} (1-\\theta)^{b-1} \\enspace. \\ d\\Theta\\\\\n",
        "&= \\binom{N}{z} \\frac{1}{B(a,b)} \\int_{0}^{1} \\theta^z (1-\\theta)^{N-z} \\enspace \\theta^{a-1} (1-\\theta)^{b-1} \\enspace\\ d\\Theta \\\\\n",
        "&=\\binom{N}{z} \\frac{1}{B(a,b)} \\int_{0}^{1} \\theta^{z+a-1} (1-\\theta)^{N-z+b-1} \\enspace\\ d\\Theta \\\\\n",
        "\n",
        "&=\\binom{N}{z}\\,\\frac{B\\bigl(z+a,\\;N-z+b\\bigr)}{B(a,b)}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRj5UCqR3MNN"
      },
      "source": [
        "2. Similarly, derive the expression for the marginal likelihood of $m_0$, $p(z \\mid N, m_0)$ in terms of $c, z, N$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0hCrxLR3X70"
      },
      "source": [
        "_ANSWER:_\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(z\\mid N,m_0)\n",
        "&= \\int_{0}^{1}p(z\\mid N,\\theta)\\,p(\\theta\\mid c)\\,\\mathrm d\\theta \\\\[6pt]\n",
        "&= \\int_{0}^{1}\n",
        "   \\binom{N}{z}\\,\\theta^{z}(1-\\theta)^{N-z}\n",
        "   \\;\\text{spike}(\\theta|c)\n",
        "   \\,\\mathrm d\\theta \\\\[6pt]\n",
        "&= \\binom{N}{z}\\,c^{z}(1-c)^{N-z}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsLKqBsz2R-_"
      },
      "source": [
        "__Practical note:__\n",
        "\n",
        "You are almost ready to compute the Bayes factor. However, there is one important caveat: some terms in the marginal likelihood can result in numerical issues. For example $\\binom{238}{187}\\approx 3.19 \\times 10^{52}$, and when working with such huge numbers, we might run into numerical under- and overflow. In practice, we therefore often compute the _log_ marginal likelihoods and _log_ Bayes factor. From basic algebra it follows that\n",
        "\n",
        "$$\n",
        " \\log BF_{10} = \\log \\left(\\frac{p(z \\mid N, m_1)}{p(z \\mid N, m_0)} \\right) = \\log p(z \\mid N, m_1) - \\log p(z \\mid N, m_0) \\enspace.\n",
        "$$\n",
        "\n",
        "To proceed, we need to determine the logarithm of both of the marginal likelihoods, that is $\\log p(z \\mid N, m_1)$ and $\\log p(z \\mid N, m_0)$. Here are some hints for this derivation as well as the implementation for the next question:\n",
        "\n",
        "- We can rewrite \n",
        "$$\n",
        "    \\log \\binom{N}{z} = -\\log(N+1) - \\log(B(N-z+1, z+1)) \\enspace.\n",
        "$$\n",
        "- $\\log B(a, b)$ is implemented in `scipy.special.betaln(a, b)`. \n",
        "\n",
        "3. Derive the expressions for the _log_ marginal likelihoods for both models (see https://en.wikipedia.org/wiki/Logarithm if you need a refresher on logarithms)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gktwcmgC9mQ2"
      },
      "source": [
        "_ANSWER:_\n",
        "\n",
        "Model 0:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log p(z\\mid N,m_0)\n",
        "&= \\log\\binom{N}{z} + z\\log c + (N-z)\\log(1-c) \\\\[6pt]\n",
        "&= -\\log(N+1) \\;-\\;\\log B(N-z+1,\\;z+1) \\;+\\; z\\log c \\;+\\;(N-z)\\log(1-c).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Model 1:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log p(z\\mid N,m_1)\n",
        "&= \\log\\binom{N}{z} \\;+\\; \\log B\\bigl(z+a,\\;N-z+b\\bigr)\\;-\\;\\log B(a,b) \\\\[6pt]\n",
        "&= -\\log(N+1)\\;-\\;\\log B(N-z+1,\\;z+1)\\;+\\;\\log B\\bigl(z+a,\\;N-z+b\\bigr)\\;-\\;\\log B(a,b).\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz-y1V0k-qDN"
      },
      "source": [
        "4. Now implement these computations using `Numpy` and `scipy.special.betaln`. Determine the log marginal likelihoods, and with these, compute $\\log BF_{10}$ and $BF_{10}$ (which is of course $\\exp\\log BF_{10}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2ed7_GhA0pq"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HIemi_3_xgPp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bayes factor (m1 vs m0): 57961969258180344.000\n"
          ]
        }
      ],
      "source": [
        "from scipy.special import betaln\n",
        "\n",
        "N = 238\n",
        "z = 187\n",
        "a = b = 1\n",
        "c = 0.5\n",
        "\n",
        "log_margin_likelihood_m0 = (\n",
        "    -np.log(N + 1)\n",
        "    - betaln(N - z + 1, z + 1)\n",
        "    + z * np.log(c)\n",
        "    + (N - z) * np.log(1 - c)\n",
        ")\n",
        "\n",
        "log_margin_likelihood_m1 = (\n",
        "    -np.log(N + 1)\n",
        "    - betaln(N - z + 1, z + 1)\n",
        "    + betaln(z + a, N - z + b)\n",
        "    - betaln(a, b)\n",
        ")\n",
        "\n",
        "bf_10 = np.exp(log_margin_likelihood_m1 - log_margin_likelihood_m0)\n",
        "print(f'Bayes factor (m1 vs m0): {bf_10:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj34ZILz_FEF"
      },
      "source": [
        "5. Consult the Bayes factor interpretation table (slide 23, Lecture 7). What is the conclusion from this little experiment? Does it match your intuition?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBzh6883AXTc"
      },
      "source": [
        "_ANSWER:_ The result is roughly $5.8 \\times 10^{16}$. Considering that we are comparing 238 trials with 187 successful trials versus a 50%/50%, such decisive evidence makes sense. In other words, the likelihood of observing so many successes with a fair coin in so many trials is virtually impossible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elnuL0Pk0eH6"
      },
      "source": [
        "## 1.2 Comparison with another country\n",
        "\n",
        "After these promising results, DentoCare now also releases the Dentinator 2000 in Spain. For complicated political reasons, DentoCare wonders whether there is a difference in reception of the new toothbrush between France ($F$) and Spain ($S$), so they poll in Spain as well. They find $N_S = 338$ and $z_S=229$ (compared to $N_F=238, z_F=187$). \n",
        "\n",
        "Again, DentoCare uses Bayesian model comparison for their analysis. They want to find out whether Spain and France respond differently to the new toothbrush. In their null model, $m_0$, there is no difference, which means $\\theta_F = \\theta_S$. In the alternative model, $m_1$, there _is_ a difference, which means $\\theta_F \\neq \\theta_S$. \n",
        "\n",
        "There are several ways we could implement this model comparison, but an exact solution will be difficult. For this exercise, we shall use the approach with the categorical model variable `m` in JAGS.\n",
        "\n",
        "1. First, write down the generative models for $m_0$ and $m_1$. Pick convenient (conjugate) likelihoods and (uninformative) priors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyt8Fw-8ejze"
      },
      "source": [
        "_ANSWER:_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IRsfZktfIRH"
      },
      "source": [
        "2. Now implement the model comparison in JAGS. See slide #33, Lecture 7 for the outline of how to do this. Hint: you can use the fact that in $m_0$, $\\theta_S$ and $\\theta_F$ are the same. That means you can sample one, and then _set_ the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_iYFUFVgJlz"
      },
      "source": [
        "_ANSWER:_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcSkUGTvIUhc"
      },
      "outputs": [],
      "source": [
        "jags_model = '''\n",
        "model {\n",
        "    ## Prior\n",
        "    \n",
        "\n",
        "    ## Likelihood\n",
        "}\n",
        "'''\n",
        "\n",
        "N_F = 238\n",
        "z_F = 187\n",
        "\n",
        "N_S = 338\n",
        "z_S = 229\n",
        "\n",
        "a = 1\n",
        "b = 1\n",
        "\n",
        "data = dict(...)\n",
        "\n",
        "num_samples = 100000\n",
        "num_chains = 4\n",
        "m = pj.Model(jags_model, data=data, chains=num_chains)\n",
        "\n",
        "samples = m.sample(num_samples, vars=[...])\n",
        "print('Sampling complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6hDtMZPgvUn"
      },
      "source": [
        "3. Compute the posterior model probabilities and the Bayes factor of this comparison. Interpret the Bayes factor according to the interpretation table. What do you conclude about the difference between French and Spanish dentists?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8i1Kg12iaeY"
      },
      "source": [
        "_ANSWER:_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CobSYONLCV8v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFOeiwWRjSWG"
      },
      "source": [
        "# 1.3 Another method for the same goal\n",
        "\n",
        "Interestingly, there are multiple ways to Rome. With the same models, but a slightly different variation, we can perform another model comparison. Time to test whether they are consistent (hopefully!).\n",
        "\n",
        "Let us first reformulate alternative model used earlier, $m_1$, to explicitly learn the difference in preference between France and Spain, $\\delta=\\theta_F-\\theta_S$. Note that in the null model, $\\delta=0$ by design! This means we can use the Savage-Dickey approach, as the null model is a special case of $m_1$, where $\\delta$ is exactly 0. \n",
        "\n",
        "Using the Savage-Dickey approach, we can compute the Bayes factor as follows:\n",
        "\n",
        "$$\n",
        "    BF_{10} = \\frac{p(D \\mid m_1)}{p(D \\mid m_0)} = \\frac{p(\\delta=\\delta_0 \\mid m_1)}{p(\\delta=\\delta_0 \\mid D, m_1)} \\enspace,\n",
        "$$ \n",
        "\n",
        "where $\\delta_0$ is the value implied by the null model. In our dentistry case, the null model states that $\\delta=\\theta_F - \\theta_S = 0$. \n",
        "\n",
        "Note that we have not specified a prior on $\\delta$ -- at least, not explicitly. However, if we add $\\delta=\\theta_F-\\theta_S$ to our generative model and our JAGS implementation, then if we do not provide observations ($z$) to our model, we simply sample from the prior. This way, we can obtain samples from $\\delta \\sim p(\\delta=\\delta_0 \\mid m_1)$. If we do add data $z$ to condition on, then we obtain samples from $\\delta \\sim p(\\delta=\\delta_0 \\mid D, m_1)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsJ9bBhwmvT6"
      },
      "source": [
        "1. Implement model $m_1$ with the explicit sampling of $\\delta=\\theta_F - \\theta_S$. Collect samples from the prior, and collect samples from the posterior. Visualize both a histograms within one figure. \n",
        "\n",
        "Tip: you can choose to run the model twice, once conditioned on data, the other time not, but you can also add both the same model definition, which is a bit cleaner. Here is an example of how you could do this for a simple coin flip model. In JAGS you would write:\n",
        "\n",
        "```\n",
        "theta ~ dbeta(a,b)\n",
        "theta.prior ~ dbeta(a,b)\n",
        "x ~ dbern(theta)\n",
        "```\n",
        "\n",
        "and then in Python you'd write:\n",
        "\n",
        "```\n",
        "samples = m.sample(num_samples, vars=['theta', 'theta.prior'])\n",
        "```\n",
        "\n",
        "This would give you samples from the posterior (`theta`) and from the prior (`theta.prior`) in one go.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh2tufian-Ox"
      },
      "source": [
        "_ANSWER:_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll0uV5EUm_RN"
      },
      "outputs": [],
      "source": [
        "jags_model = '''\n",
        "model {\n",
        "    ## Prior\n",
        "\n",
        "\n",
        "    ## Likelihood\n",
        "}\n",
        "'''\n",
        "\n",
        "N_F = 238\n",
        "z_F = 187\n",
        "\n",
        "N_S = 338\n",
        "z_S = 229\n",
        "\n",
        "a = 1\n",
        "b = 1\n",
        "\n",
        "data = dict(...)\n",
        "\n",
        "num_samples = 100000\n",
        "num_chains = 4\n",
        "m = pj.Model(jags_model, data=data, chains=num_chains)\n",
        "\n",
        "samples = m.sample(num_samples, vars=[...])\n",
        "print('Sampling complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngyVP68Wn5E5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "ax = plt.gca()\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sBKSqIwn9JA"
      },
      "source": [
        "In order to compute the Savage-Dickey density ratio, we need to obtain the probabilities indicated at the value $\\delta=0$ in these histograms. However, reading that with the naked eye from histograms is not particularly accurate. Instead, we shall use _kernel density estimation_. This technique smooths the histogram, but also allows us to query the probability density function that the histogram approximates, at any location of our choice. The function we use is `scipy.stats.gaussian_kde` (see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html). The term 'Gaussian' has to do with how it smooths the histogram, not with the distributions we use in our model!\n",
        "\n",
        "2. Determine the PDFs of the posterior and the prior distributions $p(\\delta \\mid m_1)$ by applying the `gaussian_kde` function on the samples from your JAGS model. Plot these PDFs as a function from the whole possible range of $\\delta$ ($\\delta \\in [-1, 1]$). Add a black dot at the points where the prior and the posterior intersect the vertical line where $\\delta=0$, as implied by $m_0$ (you can add such a dot with `ax.plot(..., ..., 'o', color='k')`.\n",
        "\n",
        "Note: The `p = gaussian_kde` function returns another function. This function is the approximated PDF, based on the samples you used. You can query `p` for example like `p([0.1, 0.2, 0.3])` or `p(np.linspace(0, 1, num=40)`. If you want to query it at one specific number, write: `p([number])` (note the square brackets)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omzlRe4fpzj3"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEN2yCRto0BP"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "posterior = gaussian_kde(s...)\n",
        "prior = gaussian_kde(...)\n",
        "\n",
        "# the resolution of the linspace determines how smooth the plots are\n",
        "delta_min, delta_max = -1.0, 1.0\n",
        "delta_range = np.linspace(delta_min, delta_max, num=500)\n",
        "delta_according_to_m0 = ...\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = plt.gca()\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R3fR_t2p1Xn"
      },
      "source": [
        "3. Now use the values at these black dots to compute the Bayes factor of the comparison using the Savage-Dickey approach. Is it close to the value you found with JAGS (let's call it close if they are within 10% of each other) in question 1.2.3?\n",
        "\n",
        "Remember that both approaches are based on MCMC sampling; they are not exact. If you find results that are roughly within the same ballpark, but further than 10% apart, you might simply want to increase the number of JAGS samples. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdtAI4vcq_48"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7CN39Qqp_DZ"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGm1_xqC1FYc"
      },
      "source": [
        "We saw in the lectures that in order to compute Bayes factors, we need marginal likelihoods, and that these are very sensitive to the prior distribution in a model. That applies here as well. \n",
        "\n",
        "Here, we explore what happens to the Bayes factor if we change the prior from extremely weak (uninformative) to extremely strong.\n",
        "\n",
        "4. First, use the code template below to plot the prior and posterior for $m_1$ for different $q$ values of $a$ and $b$ (for simplicity, we keep $a=b$ here). Note that a `logspace` is used; so we explore values starting at 1, and ending with 1000, in logarithmically spaced steps. Explain what happens to the prior, and what happens to the posterior, as $a$ and $b$ are increased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXqjm6HXvi1j"
      },
      "source": [
        "_ANSWER:_ \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbj9BxGX1aRQ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "num_samples = 50000\n",
        "num_chains = 4\n",
        "\n",
        "q = 20\n",
        "as_ = np.logspace(0, 3, num=q)\n",
        "bs_ = np.logspace(0, 3, num=q)\n",
        "\n",
        "delta_according_to_m0 = ...\n",
        "\n",
        "_, axes = plt.subplots(nrows=4, ncols=5, sharex=True, sharey='col', \n",
        "                       constrained_layout=True, figsize=(20, 12))\n",
        "\n",
        "for i, (a_, b_) in enumerate(zip(as_, bs_)):\n",
        "    ax = axes.flatten()[i]  # plot in this axis\n",
        "\n",
        "    data = dict(...)\n",
        "    \n",
        "    m_ = pj.Model(jags_model, data=data, chains=num_chains)\n",
        "    samples_ = ...\n",
        "    posterior_ = ...\n",
        "    prior_ = ...\n",
        "    \n",
        "    # do the plotting here:\n",
        "\n",
        "plt.suptitle('The effect of different prior strengths')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnR4G7H-vzj4"
      },
      "source": [
        "5. Now make a single figure in which you plot the Bayes factor of this model comparison as a function of $a$ ($=b$) on the x-axis. You should see an initial increase in the Bayes factor, but then the Bayes factor decays to some asymptotic value. What is this value, and why does it make sense that the Bayes factor converges here, as we increase the strength of the prior? What does this mean for the model comparison?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7zpn9KvwTwE"
      },
      "source": [
        "_ANSWER_:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8XtLK0l2Q4o"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "ax = plt.gca()\n",
        "ax.plot(as_, bfs)  # `bfs` is what you have to determine\n",
        "ax.set_xlabel(r'$a, b$')\n",
        "ax.set_ylabel(r'$BF_{10}$')\n",
        "ax.set_xlim([1, 1000]);\n",
        "ax.set_title('The Bayes factor as a function of $a=b$');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho-Y-72OzQrl"
      },
      "source": [
        "---\n",
        "# 2. Time for a drink\n",
        "\n",
        "After all this analysis for DentoCare, it is time to set our sights on a more enjoyable topic: booze. In this part of the exercise we'll study the quality of red wine ($y$), as a function of its chemical properties ($x$), such as pH, alcohol content, citric acid levels, and so on.\n",
        "\n",
        "Our data set consists of $N=1599$ wines, and for each we have observed $p=11$ of these properties.\n",
        "\n",
        "We'll try to figure out, using Bayesian testing, which of these properties are important for predicting wine quality, and which are not. To do so, we make use of the generalized linear model again, and describe our model as:\n",
        "\n",
        "$$\n",
        "    \\begin{align}\n",
        "        \\beta_j &\\sim \\mathcal{N}(\\beta_j \\mid \\mu_{\\beta}, \\sigma_{\\beta}) && j=0, \\ldots, p\\\\\n",
        "        y_i \\mid \\mathbf{x}_i, \\mathbf{\\beta} &\\sim \\mathcal{N}(y_i \\mid \\beta_0 + \\mathbf{x}_i^\\top \\mathbf{\\beta}, \\sigma_y) && i =1,\\ldots, N\\enspace,\n",
        "    \\end{align}\n",
        "$$\n",
        "\n",
        "Importantly, the quality of the wine is a linear combination of the chemical properties and the coefficients $\\mathbf{\\beta}$. Note that $\\beta$ is a vector of $p+1$ elements, as it also contains the intercept $\\beta_0$.\n",
        "\n",
        "The code below loads the data, and makes a figure of the distribution of the feature and outcome values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej3p-WaRzZYK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "filename = 'winequality-red.csv'\n",
        "\n",
        "df = pd.read_csv(filename, sep=';')\n",
        "feature_names = df.columns\n",
        "x = df.values[:, 0:-1]\n",
        "y = df.values[:, -1]\n",
        "n, p = x.shape\n",
        "\n",
        "_, axes = plt.subplots(nrows=2, ncols=6, sharey=True, constrained_layout=True, \n",
        "                       figsize=(18, 6))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    if i < p:\n",
        "        ax.hist(x[:, i], bins=30)\n",
        "    else:\n",
        "        ax.hist(y, bins=30)\n",
        "    ax.set_title(feature_names[i], fontsize=14)\n",
        "\n",
        "ax = axes.flatten()[-1]\n",
        "for _, spine in ax.spines.items():\n",
        "    spine.set_color('red')    \n",
        "    spine.set_color('red')\n",
        "\n",
        "plt.suptitle('Unprocessed distributions of observed values', fontsize=20);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MnZ4sV3J_sN"
      },
      "source": [
        "**Practical note**: Notice how the different features have very different _ranges_ of possible values. For instance, the citric acid ranges from 0 to 0.7 (ish), whereas the alcohol percentage is in the range 9--14. This makes it hard to compare the regression coefficients $\\beta$ with each other, as these coefficients will have to adjust to the scales of the corresponding predictor. For example, suppose the pH increases from 0.5 to 0.6, and let $\\beta_{\\text{pH}}=3.2$. This means a change in $y$ of $3.2(0.6-0.5) = 0.32$. If $\\beta_{\\text{alcohol}}=3.2$, and alcohol increases from 12% to 13%, then the change in $y$ is $3.2(13-12)=3.2$. A much bigger change in $y$, even though relatively speaking the predictors didn't change that much differently.\n",
        "\n",
        "The standard data science approach to address this, is to standardize our input, so that every predictor has mean 0 and standard deviation 1. We shall adopt this approach here. \n",
        "\n",
        "1. Preprocess the features $x$ by, for each column of $x$, subtracting the mean $\\mu(x)$ and dividing by the standard deviation $\\sigma(x)$. Make another figure like the one above, but show the scaled versions of $x$. Make sure that indeed the mean of each histogram is now at 0. Do the same for the last observed value, the target wine quality $y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQfOs-s4SpYd"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLwGDb0IR8QN"
      },
      "outputs": [],
      "source": [
        "x = df.values[:, 0:-1]\n",
        "x = ... # preprocess\n",
        "y = df.values[:, -1]\n",
        "y = ... # preprocess\n",
        "n, p = x.shape\n",
        "\n",
        "_, axes = plt.subplots(nrows=2, ncols=6, sharey=True, constrained_layout=True, \n",
        "                       figsize=(18, 6))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    if i < p:\n",
        "        ax.hist(x[:, i], bins=30)\n",
        "    else:\n",
        "        ax.hist(y, bins=30)\n",
        "    ax.set_title(feature_names[i], fontsize=14)\n",
        "\n",
        "\n",
        "ax = axes.flatten()[-1]\n",
        "for _, spine in ax.spines.items():\n",
        "    spine.set_color('red')    \n",
        "    spine.set_color('red')\n",
        "\n",
        "plt.suptitle('Preprocessed distributions of observed values', fontsize=20);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRfqszPRSnTh"
      },
      "source": [
        "Now we can start constructing a Bayesian multiple regression model. \n",
        "\n",
        "**Practical note**: Clearly $y$ does not follow a Gaussian distribution, even after our transformation. For simplicity, we shall stick to Gaussians, but if you're using this exercise to buy your next wine, be warned that better modelling choices are available.\n",
        "\n",
        "You have already done part of this in Large Assignment 1, and you are free to re-use your (**own!**) code from that assignment -- there is no need to reinvent the wheel if you are happy with that solution. However, we have several more predictors now though, so a linear algebra expression such as `inprod(...)` will be needed to make robust code.\n",
        "\n",
        "2. Implement a Bayesian multiple linear regression model that predicts wine quality from the observed features, with the prior and likelihood as mentioned here. Make a figure showing for each of the 11 predictors $\\beta_j, j=1, \\ldots, p$ as well as intercept $\\beta_0$ the approximate posterior distribution. Be sure to use the `inprod(...)` approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivciri4cne6p"
      },
      "source": [
        "_ANSWER:_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXdhY7VAUck0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "jags_model = '''\n",
        "model{      \n",
        "  # Prior \n",
        " \n",
        "\n",
        "  # Likelihood\n",
        "}\n",
        "'''\n",
        "\n",
        "data = dict(...)\n",
        "\n",
        "num_samples = 20000\n",
        "num_chains = 4\n",
        "m = pj.Model(jags_model, data=data, chains=num_chains)\n",
        "\n",
        "samples = m.sample(num_samples, vars=[...])\n",
        "print('Sampling complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXTfPfNrVSTV"
      },
      "outputs": [],
      "source": [
        "def get_hdi(s, intv=0.95):\n",
        "    # probably useful for the next question...\n",
        "    return ...\n",
        "\n",
        "#\n",
        "_, axes = plt.subplots(nrows=2, ncols=6, sharey=True, constrained_layout=True, \n",
        "                       figsize=(18, 6))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    ...\n",
        "\n",
        "for ax in axes[1, :]:\n",
        "    ax.set_xlabel(r'$\\beta$')\n",
        "for ax in axes[:, 0]:\n",
        "    ax.set_ylabel(r'$p(\\beta \\mid X, y)$')\n",
        "\n",
        "plt.suptitle('Approximated posterior distribution', fontsize=20);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua00tic4fcJq"
      },
      "source": [
        "The question of 'which feature is important in predicting wine quality?' is also a form of model comparison / hypothesis testing. In a way, we are comparing $2^{11}$ models here -- because each of the $p=11$ predictors can either be important or not.\n",
        "\n",
        "3. Use your posterior distributions to determine the 95% HDI for each of the predictors, and make a new figure where you show this interval on top of your posterior distributions. You can use, for example, the `ax.axvspan(..., alpha=0.2)` code. In addition, include a vertical line (with a clearly distinguishable color/linestyle) that represents for each predictor the null hypothesis that says this predictor is _not_ important (think about that this means for the corresponding $\\beta$). According to this analysis, which features are important? Explain how you got to your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NG1KYhkg_k4"
      },
      "source": [
        "_ANSWER:_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc2ihFxnA3sx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDBFRlNDmxZF"
      },
      "source": [
        "Your analysis has told us which features are important in predicting wine quality. However, a critical remark is that our choice of prior might affect our results.\n",
        "\n",
        "4. Verify the robustness of your conclusion by changing the scale (=variance/standard deviation/precision) of the prior distribution on the regression coefficients. Do you find quantitatively different results (e.g., different conclusions)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMhRQfskoTv4"
      },
      "source": [
        "_ANSWER:_ "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dummkopf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
