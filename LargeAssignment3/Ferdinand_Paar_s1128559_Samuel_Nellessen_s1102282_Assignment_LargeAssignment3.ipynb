{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbOLdxWCNpZb"
      },
      "source": [
        "# Large assignment 3\n",
        "\n",
        "## Read before you start\n",
        "\n",
        "* Provide clear and complete answers in code blocks or markdown. You may add as many as you need.\n",
        "* Always motivate your answers. This can be done in markdown cells, or in comments in code.\n",
        "* Submit your results via Brightspace. Use the following filename convention: ``StudentName1_snumber1_StudentName2_snumber2_LargeAssignment3.ipynb``.\n",
        "* Make sure you submit a fully executed version of the notebook file. The teaching assistants will not run/debug your code during grading.\n",
        "* Questions? Ask them during the workgroups, or see Brightspace for instructions on how to proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUGIfKtQNlEa",
        "outputId": "09d57659-52b8-49c4-dba3-c22f88262e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "jags is already the newest version (4.3.2-1.2004.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyjags in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: deepdish in /usr/local/lib/python3.10/dist-packages (from pyjags) (0.3.7)\n",
            "Requirement already satisfied: arviz in /usr/local/lib/python3.10/dist-packages (from pyjags) (0.15.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyjags) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from arviz->pyjags) (4.5.0)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from arviz->pyjags) (1.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from arviz->pyjags) (23.1)\n",
            "Requirement already satisfied: matplotlib>=3.2 in /usr/local/lib/python3.10/dist-packages (from arviz->pyjags) (3.7.1)\n",
            "Requirement already satisfied: setuptools>=60.0.0 in /usr/local/lib/python3.10/dist-packages (from arviz->pyjags) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from arviz->pyjags) (1.10.1)\n",
            "Requirement already satisfied: xarray>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from arviz->pyjags) (2022.12.0)\n",
            "Requirement already satisfied: h5netcdf>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from arviz->pyjags) (1.1.0)\n",
            "Requirement already satisfied: xarray-einstats>=0.3 in /usr/local/lib/python3.10/dist-packages (from arviz->pyjags) (0.5.1)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.10/dist-packages (from deepdish->pyjags) (3.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from h5netcdf>=1.0.2->arviz->pyjags) (3.8.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2->arviz->pyjags) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2->arviz->pyjags) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2->arviz->pyjags) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2->arviz->pyjags) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2->arviz->pyjags) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2->arviz->pyjags) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2->arviz->pyjags) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->arviz->pyjags) (2022.7.1)\n",
            "Requirement already satisfied: cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from tables->deepdish->pyjags) (0.29.34)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from tables->deepdish->pyjags) (2.8.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from tables->deepdish->pyjags) (9.0.0)\n",
            "Requirement already satisfied: blosc2~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from tables->deepdish->pyjags) (2.0.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from blosc2~=2.0.0->tables->deepdish->pyjags) (1.0.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2->arviz->pyjags) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!apt install jags\n",
        "!pip install pyjags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRMZw2sXarbE",
        "outputId": "be972aa5-8f3e-4235-9764-dd3b740bf425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyJAGS v1.3.8\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pyjags as pj\n",
        "\n",
        "print('Using PyJAGS v{:s}'.format(pj.__version__))\n",
        "\n",
        "plt.rc('axes', titlesize=18)        # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=18)        # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=14)       # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=14)       # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=12)       # legend fontsize\n",
        "plt.rc('figure', titlesize=30)      # fontsize of the figure title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RidR6wX6LL2X"
      },
      "source": [
        "# 1. Ageing and cognitive decline\n",
        "\n",
        "One of the topics that is studied extensively at the Donders Institute is the effect of ageing on cognition (for example, if you are interested: https://www.ru.nl/donders/research/theme-3-plasticity-memory/research-groups-theme-3/cognitive-aging/). It is generally accepted that (on average), performance on cognitive tasks decreases as we grow older. In this exercise, we'll use Bayesian modelling and Bayesian model comparison to explore this further. Note: although the data were generated specifically for this exercise (that is, they are not real measurements), the setting is not unrealistic.\n",
        "\n",
        "One of the scores that measures cognitive performance is the [Reyâ€“Osterrieth Complex Figure](https://en.wikipedia.org/wiki/Rey%E2%80%93Osterrieth_complex_figure) (ROCF) assessment, which asks participants to redraw a complicated line drawing from memory. In this assignment, we'll assume the ROCF score has been measured and stored for different people in the same age range ($50-80$), but for two different groups: The first is the `control` group, which consists of regularly ageing people. The second group is a group that has been instructed, starting at age 50, to exercise at least 2 times a week. We will refer to this group as the `active` group.\n",
        "\n",
        "The question is of course: __Does an active lifestyle reduce the effects of ageing on cognitive decline?__\n",
        "\n",
        "The following code snippet loads and plots the ROCF scores for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myey4RU-V-Uu"
      },
      "outputs": [],
      "source": [
        "data = np.load('ROCF_data.npz')\n",
        "x_control = data['x_control']\n",
        "x_active = data['x_active']\n",
        "y_control = data['y_control']\n",
        "y_active = data['y_active']\n",
        "\n",
        "labels = ['control', 'active']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = plt.gca()\n",
        "colors = ['#E66100', '#5D3A9B']\n",
        "\n",
        "ax.plot(x_control, y_control, 'o', label=labels[0], color=colors[0])\n",
        "ax.plot(x_active, y_active, 'o', label=labels[1], color=colors[1])\n",
        "ax.set_xlabel('Age')\n",
        "ax.set_ylabel('ROCF score')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yxukfu5ZI4u"
      },
      "source": [
        "The approach to study this problem will, once more, be that of linear regression. Within that context, the research question translates into: __Is the difference between the slopes in linear regression of the control group, $\\beta_1^c$ and the active group, $\\beta_1^a$, nonzero?__\n",
        "\n",
        "By looking at the data, there does appear to be a difference, although near the beginning (e.g. ages 50-60) there is still a lot of overlap.\n",
        "\n",
        "Let's call this difference between the slopes $\\delta=\\beta_1^a - \\beta_1^c$.\n",
        "\n",
        "1. What is the null hypothesis ($m_0$), in terms of $\\delta$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1nww9VYa3oT"
      },
      "source": [
        "_ANSWER:_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU6nE9gdbNqL"
      },
      "source": [
        "In our alternative hypothesis ($m_1$), the difference in slopes $\\delta$ is unknown.\n",
        "\n",
        "2. Write down a generative model that uses all observations $(x_i^c, y_i^c)_{i=1}^{n^c}$ and $(x_i^a, y_i^a)_{i=1}^{n^a}$ to learn the distribution over $\\delta$. Note: this is simply two separate linear regressions, with the addition of the $\\delta$ term. The superscripts $\\cdot^a$ and $\\cdot^c$ are used to indicate the active and the control group, respectively. As always, be mindful of which distributions make sense for which variable (and note that $\\delta$ is a _deterministic_ variable, and hence is not assigned a distribution!). Check the examples of linear regression in Lecture 6 if you are unsure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8hUg5qdccgw"
      },
      "source": [
        "_ANSWER:_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Xz-AvDdw0O"
      },
      "source": [
        "3. Draw the graphical model that corresponds to your generative model. Make sure to pay attention to the correct repetition/indexing/shading and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DjIYHzTpvYy"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dekEP5vAatQa"
      },
      "source": [
        "As we saw in Large Assignment #2 and Lectures 7 & 8, there are multiple ways of figuring out the answer to this question. This time, we shall use the Region of Practical Equivalence (ROPE) approach.\n",
        "\n",
        "4. The first step is to implement the model in JAGS and see if everything went alright - that is, we can once more plot the posterior expectations of $\\mu_i^g = \\beta_0^g + \\beta_1^g x_i^g$, with $g\\in\\{a, c\\}$. These expectations should be pretty good fits through the observed data points.\n",
        "\n",
        "Much of the code for the visualization was already provided in Lecture 6 and Large Assignment 1, and is partially repeated below for your convenience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA6aFrdhleGx"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7R6uk5qOJJb"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "code = '''\n",
        "model{\n",
        "    for (k in 1:2) {\n",
        "        ...\n",
        "    }\n",
        "\n",
        "    for (i in 1:n_control) {\n",
        "        ...\n",
        "    }\n",
        "\n",
        "    for (i in 1:n_active) {\n",
        "        ...\n",
        "    }\n",
        "    delta <- ...\n",
        "}\n",
        "'''\n",
        "\n",
        "data = dict(x_control=x_control,\n",
        "            x_active=x_active,\n",
        "            y_control=y_control,\n",
        "            y_active=y_active,\n",
        "            n_control=len(x_control),\n",
        "            n_active=len(x_active))\n",
        "num_samples = 10_000\n",
        "num_chains = 4\n",
        "\n",
        "m = pj.Model(code=code, data=data, chains=num_chains)\n",
        "samples = m.sample(num_samples, vars=[...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AklGncqlReIA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.plot(x_control, y_control, 'o', color=colors[0])\n",
        "ax.plot(x_active, y_active, 'o', color=colors[1])\n",
        "\n",
        "m = 100\n",
        "x_range = np.linspace(50, 85, num=m)\n",
        "# Rename the variables to match your JAGS code!\n",
        "\n",
        "w0_samples = np.reshape(samples['w0'], newshape=(2, num_samples*num_chains))\n",
        "w1_samples = np.reshape(samples['w1'], newshape=(2, num_samples*num_chains))\n",
        "\n",
        "x_repeat = np.repeat(x_range[:, np.newaxis], num_samples*num_chains, axis=1).T\n",
        "\n",
        "for k in range(2):\n",
        "    w0_repeat = np.repeat(w0_samples[k][:, np.newaxis], m, axis=1)\n",
        "    w1_repeat = np.repeat(w1_samples[k][:, np.newaxis], m, axis=1)\n",
        "    mu_samples = w0_repeat + w1_repeat*x_repeat\n",
        "\n",
        "    mu_exp = ...\n",
        "    mu_lower = ...\n",
        "    mu_upper = ...\n",
        "\n",
        "    ax.plot(x_range, mu_exp, color=colors[k],\n",
        "            label=f'{labels[k]} group')\n",
        "    ax.fill_between(x_range,\n",
        "                    mu_lower,\n",
        "                    mu_upper, color=colors[k], alpha=0.3)\n",
        "ax.legend()\n",
        "ax.set_xlabel('Age')\n",
        "ax.set_ylabel('Cognitive performance')\n",
        "ax.set_xlim([x_range[0], x_range[-1]]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP440fBNjErD"
      },
      "source": [
        "Once you are happy with the training of your model, we can continue with finding the answer to the research question. However, this time we have chosen the ROPE approach to model comparison / hypothesis testing. It is important that we define the ROPE __before__ we actually look at the posterior distribution of $\\delta$, because otherwise we could just conveniently pick the ROPE to match our political agenda. __THIS. IS. VERY. BAD.__\n",
        "\n",
        "To choose a ROPE, we have to figure out what we consider 'practically equivalent' with the null hypothesis value (which you determined in question #1). First, we need to have a bit of an intuition of what kind of values we can expect for the slopes $b_1^a$ and $b_1^c$, because that will also inform us about what a sensible difference is. By just looking at the data points, we can see that, for example, the control group drops about 6 ROCF points when age increases by 30, which gives us an educated guess of $\\beta_1^c\\approx 6/30=0.2$.\n",
        "\n",
        "5. Determine your choice of the ROPE (the bounds of the interval around the value indicated by the null hypothesis). Note: there is not really a right answer here, but many answers could be wrong (for example, a ROPE of $[25, 30]$ seems weird to me). The point is that you think about what values for $\\delta$ you expect, and consequently which values you think are pretty much equivalent to the null hypothesis value.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W37I-37st9E"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmH8Bw9rq8lf"
      },
      "outputs": [],
      "source": [
        "ROPE = [...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbo9WNIRmUJL"
      },
      "source": [
        "5. Make a figure showing the posterior distribution of the variable we are interested in, that is, show $p(\\delta \\mid \\mathbf{x}^c, \\mathbf{y}^c, \\mathbf{x}^a, \\mathbf{y}^a)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdtRawkfqRzT"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ib5NDtBqS6G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjqKAa1lrswO"
      },
      "source": [
        "6. Determine the 95% HDI of $p(\\delta \\mid \\mathbf{x}^c, \\mathbf{y}^c, \\mathbf{x}^a, \\mathbf{y}^a)$ and indicate it clearly on top of your figure from the previous question (for example, with a horizontal line from the left bound of the HDI to the right bound of the HDI)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdWbk5fuqTjy"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6-suOy0qU2b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Nej78DsGK_"
      },
      "source": [
        "7. Add your ROPE interval to the figure, using `ax.axvspan(...)` and `ax.axvline(...)`. Now use slide #6 from lecture 8 to answer the original research question. Does the active lifestyle have an effect in slowing down cognitive decline?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMPWJVOOqW2o"
      },
      "source": [
        "_ANSWER:_\n",
        "\n",
        "See template below as aid for questions 5, 6, and 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Lw_nM7xlgUA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.hist(..., color='tab:grey', density=True, bins=50, label='posterior')\n",
        "ax.set_xlabel(r'$\\delta$')\n",
        "ax.set_ylabel(r'$p(\\delta \\mid \\mathbf{x}^c, \\mathbf{y}^c, \\mathbf{x}^a, \\mathbf{y}^a)$');\n",
        "\n",
        "# plot ROPE with ax.axvline(...) and ax.axvspan(...)\n",
        "...\n",
        "\n",
        "\n",
        "# plot HDI\n",
        "hdi = ...\n",
        "ax.plot(hdi, [2, 2], color='k', lw=5, label='95% HDI')\n",
        "ax.legend(frameon=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OOGDn0Zta0l"
      },
      "source": [
        "## Note\n",
        "\n",
        "If the fact that we draw a conclusion based on a fairly arbitrary ROPE makes you uneasy, then we agree. At the same time, if we used e.g. the Bayes factor approach (through Savage-Dickey or otherwise), the decision would depend on our choices of priors. The bottom line is that everything is, in the end, subjective.\n",
        "\n",
        "A frequentist approach would implicitely assume $\\mathcal{N}(0, \\infty)$ priors on the regression coefficients, which is uninformative, but not reasonable. Neither statistical framework gets us a free lunch.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsg1yfUZulvU"
      },
      "source": [
        "# 2. CO2 measurements at the Mauna Loa observatory\n",
        "\n",
        "[The Mauna Loa observatory](https://gml.noaa.gov/obop/mlo/) is a research facility that measures all sorts of atmospheric properties, such as CO2 concentration (in parts-per-million, ppm). These data form a _timeseries_ (which is a fancy word for a regression problem where $x$ represents time).\n",
        "\n",
        "The code below loads and visualizes this famous dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJc8TqaHv9Qh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "## Load:\n",
        "data = pd.read_csv('co2_mm_mlo.txt', delim_whitespace=True,\n",
        "                   usecols=[0, 1, 2, 3], names=['Year', 'Month', 'Time', 'CO2'],\n",
        "                   decimal=',')\n",
        "x = data['Time'].to_numpy()  # in months\n",
        "y = data['CO2'].to_numpy()   # CO2 per month"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0j2nx9L2CgK"
      },
      "source": [
        "This type of timeseries often can be described by a number of different components. For instance, there is a clear _seasonal_ trend, representing the systematic fluctuations over the seasons as a kind of sine wave. In addition, there is the _upward_ trend (which is what we as a society need to worry about a lot).\n",
        "\n",
        "In terms of statistical modelling, there are a number of important topics when it comes to timeseries:\n",
        "\n",
        "* Can we predict the future?\n",
        "* What kind of trends are present in the data?\n",
        "\n",
        "First, we will take a look at the first topic, that of prediction. In order to figure out if our models make any sense, we will split the data into a _train set_ (all data up to [not including] 2010) and a _test set_ (all data starting at 2010):\n",
        "\n",
        "**Note**: I make a distinction between the actual time (1990, 1990.1, ...) $t$ and the index $x$ (1, 2, ...). In theory we could use both as our predictors, but in practice it is easier to use the index $x$, because otherwise our sampler starts so very far away from the actual posterior distribution. So keep in mind: use $x$ to train your model and make predictions, but use $t$ for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuqQQa4q3S1D"
      },
      "outputs": [],
      "source": [
        "index_of_2010 = 622  # The row in the file where year 2010 starts\n",
        "\n",
        "time_train = x[:index_of_2010]\n",
        "x_train = np.arange(len(time_train))\n",
        "y_train = y[:index_of_2010]\n",
        "\n",
        "time_test = x[index_of_2010:]\n",
        "x_test = np.arange(len(time_test)) + len(time_train)\n",
        "y_test = y[index_of_2010:]\n",
        "\n",
        "n_train = len(x_train)\n",
        "n_test = len(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGtxOciZ3qXq"
      },
      "outputs": [],
      "source": [
        "# Visualize data\n",
        "\n",
        "colors = ['#40B0A6', '#E1BE6A']\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = plt.gca()\n",
        "ax.plot(time_train, y_train, color=colors[0], label='Train data')\n",
        "ax.plot(time_test, y_test, color=colors[1], label='Test data')\n",
        "ax.axvline(x=x[index_of_2010], ls='--', color='k')\n",
        "ax.set_xlabel('Time (years)')\n",
        "ax.set_ylabel(r'$CO_2$ (ppm))')\n",
        "ax.legend()\n",
        "ax.set_xlim([np.min(x), np.max(x)]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t0ZwNrB0idP"
      },
      "source": [
        "In this exercise, you'll construct three Bayesian models that you'll train on (`x_train`, `y_train`) and evaluate using (`x_test`, `y_test`).\n",
        "\n",
        "**Note**: the models are extensions of each other; make sure the first is correct, then creating the extensions will be much easier.\n",
        "\n",
        "### The linear model\n",
        "\n",
        "The first model to try is a linear regression approach, which you have implemented/seen several times before.\n",
        "\n",
        "1. Implement a Bayesian linear regression model, and learn the (approximated) posterior distribution $p(\\beta_0, \\beta_1, \\sigma \\mid \\mathbf{x}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$. Feel free to re-use your (own!) JAGS code from previous exercises, or check the lecture slides. Make a figure with three subplots, one showing $\\beta_0$, one showing $\\beta_1$, and the last one showing $\\sigma$. Do your plots seem sensible? Make sure to keep track of $\\mu_i = \\beta_0 + \\beta_1 x_i$, you will need it later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faKSMlALsy73"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIowdlXg0hxO"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "code = '''\n",
        "model{\n",
        "    ...\n",
        "\n",
        "    for (i in 1:n_train) {\n",
        "        ...\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "data = dict(x_train=x_train,\n",
        "            y_train=y_train,\n",
        "            n_train=n_train)\n",
        "num_samples = 10_000\n",
        "num_chains = 4\n",
        "\n",
        "m_linear = pj.Model(code=code, data=data, chains=num_chains)\n",
        "samples_linear = m_linear.sample(num_samples, vars=[...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuBOuvdMBs7A"
      },
      "outputs": [],
      "source": [
        "_, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4), constrained_layout=True)\n",
        "\n",
        "axes[0].hist..., density=True, bins=50)\n",
        "axes[0].set_xlabel(r'$\\beta_0$')\n",
        "axes[0].set_ylabel(r'$p(\\beta_0 \\mid \\mathbf{x}, \\mathbf{y}) $')\n",
        "\n",
        "axes[1].hist(..., density=True, bins=50)\n",
        "axes[1].set_xlabel(r'$\\beta_1$')\n",
        "axes[1].set_ylabel(r'$p(\\beta_1 \\mid \\mathbf{x}, \\mathbf{y}) $')\n",
        "\n",
        "axes[2].hist(..., density=True, bins=50)\n",
        "axes[2].set_xlabel(r'$\\sigma$')\n",
        "axes[2].set_ylabel(r'$p(\\sigma \\mid \\mathbf{x}, \\mathbf{y}) $')\n",
        "\n",
        "plt.suptitle(r'The posterior $p(\\beta_0, \\beta_1, \\sigma \\mid \\mathbf{x}, \\mathbf{y})$');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_UjNLMKC6zs"
      },
      "source": [
        "The next task is to use this model to _predict_ what happened with the CO2 concentrations after 2010. In Bayesian terms, that means we want the posterior predictive distribution\n",
        "\n",
        "$$\n",
        "p(y^* \\mid \\mathbf{x}_{\\text{train}}, \\mathbf{y}_{\\text{train}}) = \\int\\int\\int p(y^* \\mid \\beta_0, \\beta_1, \\sigma, x^*) p(\\beta_0, \\beta_1, \\sigma \\mid \\mathbf{x}_{\\text{train}}, \\mathbf{y}_{\\text{train}})\\, \\text{d}\\beta_0 \\text{d}\\beta_1 \\text{d}\\sigma \\enspace.\n",
        "$$\n",
        "\n",
        "Here, $y^*$ indicates the CO2 concentrations at timepoint $x^*$, which is a point **not** in our training set.\n",
        "\n",
        "Of course, we will not exactly solve this difficult triple integral, but we use JAGS to make an MCMC-based approximation. This is quite straightforward:\n",
        "\n",
        "2. Copy your JAGS code to a new code cell. In your JAGS code you probably have a for loop similar to `for (i in 1:n_train) {...}`. Within this loop, you describe `y_train[i] ~ ...`. You now need to create an additional loop, but this one describes the **predicted** points. It is a loop over the points that we have not trained on: `for (j in 1:n_test) {...}`. **Important**: Do not provide `y_test` as data to JAGS, that would be cheating. Instead, sample `y_predict[j] ~ ...` and track JAGS' predictions. Your `samples['y_predict']` object should have shape `(n_test, num_samples, num_chains)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_23tH-Xs2km"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql8R7NFSq5OL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "code = '''\n",
        "model{\n",
        "    ...\n",
        "\n",
        "    for (i in 1:n_train) {\n",
        "        ...\n",
        "    }\n",
        "\n",
        "    for (j in 1:n_test) {\n",
        "        ...\n",
        "    }\n",
        "\n",
        "}\n",
        "'''\n",
        "\n",
        "# Note: do not pass y_test to the JAGS model, that would be cheating!\n",
        "data = dict(x_train=x_train,\n",
        "            y_train=y_train,\n",
        "            x_test=x_test,\n",
        "            n_train=n_train,\n",
        "            n_test=n_test)\n",
        "num_samples = 10_000\n",
        "num_chains = 4\n",
        "\n",
        "m_linear = pj.Model(code=code, data=data, chains=num_chains)\n",
        "samples_linear = m_linear.sample(num_samples, vars=[...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icgJ-zyQGLvM"
      },
      "source": [
        "3. Copy the data visualization code and plot in addition the mean estimated $\\mu_i$ over the training range. Furthermore, compute and visualize the mean and 95% HDI of the posterior predictive distribution. What is the expected CO2 concentration at the final observation (May 2021)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zE87mdrs4Ax"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF18w6Ph7Vxq"
      },
      "outputs": [],
      "source": [
        "y_predict_mean = ...\n",
        "y_predict_lb = ...  # lower bound of HDI\n",
        "y_predict_ub = ...  # upper bound of HDI\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = plt.gca()\n",
        "ax.plot(time_train, y_train, color=colors[0], label='Train data')\n",
        "ax.plot(time_test, y_test, color=colors[1], label='Test data')\n",
        "ax.plot(time_train, ..., color='tab:blue',\n",
        "        label='Train $\\mu_i$')\n",
        "\n",
        "ax.plot(time_test, ..., color='tab:red', label='Prediction')\n",
        "ax.fill_between(time_test, y_predict_lb, y_predict_ub, color='tab:red',\n",
        "                alpha=0.2)\n",
        "\n",
        "ax.axvline(x=x[index_of_2010], ls='--', color='k')\n",
        "ax.set_xlabel('Time (years)')\n",
        "ax.set_ylabel(r'$CO_2$ (ppm))')\n",
        "ax.legend()\n",
        "ax.set_xlim([np.min(x), np.max(x)]);\n",
        "\n",
        "print('Predicted CO2 on May 2021:',...);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TjQN257ZwY0"
      },
      "source": [
        "The 95% HDI shows a nice and smooth interval, but the individual samples can be quite noisy.\n",
        "\n",
        "4. Make a new figure that shows, instead of the 95% HDI, 5 randomly chosen samples of $\\mathbf{y}_{\\text{test}}$. Note that each sample is a vector of length 137, ranging from January 2010 to May 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5wPeFHks5Vo"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSLYaMY5a4pE"
      },
      "outputs": [],
      "source": [
        "y_predict_samples = ...\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = plt.gca()\n",
        "ax.plot(time_train, y_train, color=colors[0], label='Train data')\n",
        "ax.plot(time_test, y_test, color=colors[1], label='Test data')\n",
        "ax.plot(time_train, ..., color='tab:blue',\n",
        "        label='Train $\\mu_i$')\n",
        "\n",
        "# pick 5 random samples\n",
        "for i in np.random.randint(num_samples*num_chains, size=5):\n",
        "    ax.plot(time_test, ..., color='tab:red', alpha=0.3)\n",
        "\n",
        "ax.axvline(x=x[index_of_2010], ls='--', color='k')\n",
        "ax.set_xlabel('Time (years)')\n",
        "ax.set_ylabel(r'$CO_2$ (ppm))')\n",
        "ax.legend()\n",
        "ax.set_xlim([np.min(x), np.max(x)]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1QX9cVdFKtF"
      },
      "source": [
        "With the naked eye, we already know that our predictions are imperfect. We can quantify this into a (distribution of a) single number, so that we can later see whether changes to our model improve our predictions. One of the metrics to do this is the Mean Absolute Percentage Error (MAPE):\n",
        "\n",
        "$$\n",
        "  MAPE(\\mathbf{y}^*, \\mathbf{y}_{\\text{test}}) = \\frac{1}{n} \\sum_{i=1}^n \\frac{|y^*_i - y_{\\text{test},i}|}{y_{\\text{test},i}} \\enspace.\n",
        "$$\n",
        "\n",
        "**(The lower the better!)**\n",
        "\n",
        "The code below implements the calculation of the MAPE score for your predictions. The result is a distribution over MAPE scores, as we have a different score for every sample of the posterior predictive distribution.\n",
        "\n",
        "5. Create the variable `y_predict_samples` from your MCMC output if you haven't already, and make sure it has shape `(n_test, num_chains*num_samples)`, then run the code below to make the histogram of MAPE scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvT6Prues616"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTvHFBSvq7kf"
      },
      "outputs": [],
      "source": [
        "def mape(y_pred_):\n",
        "    return np.mean(np.abs(y_pred_ - y_test) / y_test)\n",
        "\n",
        "#\n",
        "\n",
        "print('Shape of y_predict_samples:', y_predict_samples.shape)\n",
        "\n",
        "# make sure y_predict_samples has the correct shape!\n",
        "mape_scores = np.apply_along_axis(mape, 1, y_predict_samples.T)\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(mape_scores, density=True, bins=50)\n",
        "plt.xlabel('MAPE')\n",
        "plt.ylabel('p(MAPE)')\n",
        "plt.title('Performance of linear model');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLTFwHxTxoq7"
      },
      "source": [
        "### The linear + seasonal model\n",
        "\n",
        "In the previous steps, we use Bayesian linear regression to determine the CO2 predictions. That means something along the lines of (omitting priors for now):\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\mu_i \\mid \\beta_0, \\beta_1, x_i &= \\beta_0 + \\beta_1 x_i && i=1,\\ldots,n\\\\\n",
        "    y_i \\mid \\mu_i, \\sigma & \\sim \\mathcal{N}(\\mu_i, \\sigma)  && i=1,\\ldots,n\\enspace.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "It is straightforward to add the seasonal component of the observations into this model. In that case, we have:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\mu_i \\mid \\beta_0, \\beta_1, \\beta_2, \\phi, x_i &= \\beta_0 + \\beta_1 x_i + \\beta_2 \\sin(2\\pi / 12 x_i + \\phi)  && i=1,\\ldots,n\\\\\n",
        "    y_i \\mid \\mu_i, \\sigma & \\sim \\mathcal{N}(\\mu_i, \\sigma)  && i=1,\\ldots,n\\enspace.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The term $\\beta_2$ is the coefficient that indicates how strong the seasonal influence is. The period of the sine wave is 12 months (hence $2\\pi/12$), and $\\phi$ is the _phase_ that tells us where the sine wave is at $x=0$. This is an additional parameter to learn, as we do not know exactly what this value should be.\n",
        "\n",
        "6. Copy your previous model into the code block below, and add the seasonal part. For $\\beta_2$, it makes sense to use the same priors as for the other regression coefficients. The phase $\\phi$ is a value between 0 and the period of the sine wave (12) - what would be a sensible prior distribution here? Run your code and make the same figure as for subquestion 3, showing the actual observations, the mean $\\mu_i$ over the range of the training data, and the posterior predictive distribution using its mean and its 95% HDI, over the range of the test data.\n",
        "\n",
        "**Note**: This probably takes a bit longer to sample!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPt25wmps8a_"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-LgqvZryoZO"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "code = '''\n",
        "model{\n",
        "    ...\n",
        "\n",
        "    for (i in 1:n_train) {\n",
        "        ...\n",
        "    }\n",
        "\n",
        "    for (j in 1:n_test) {\n",
        "        ...\n",
        "    }\n",
        "\n",
        "}\n",
        "'''\n",
        "\n",
        "data = dict(x_train=x_train,\n",
        "            y_train=y_train,\n",
        "            x_test=x_test,\n",
        "            n_train=n_train,\n",
        "            n_test=n_test)\n",
        "num_samples = 10_000\n",
        "num_chains = 4\n",
        "\n",
        "m_seasonal = pj.Model(code=code, data=data, chains=num_chains)\n",
        "samples_seasonal = m_seasonal.sample(num_samples, vars=[...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r_phYRgysr2"
      },
      "outputs": [],
      "source": [
        "y_predict_mean = ...\n",
        "y_predict_lb = ...\n",
        "y_predict_ub = ...\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = plt.gca()\n",
        "ax.plot(time_train, y_train, color=colors[0], label='Train data')\n",
        "ax.plot(time_test, y_test, color=colors[1], label='Test data')\n",
        "ax.plot(time_train, ...,\n",
        "        color='tab:blue', label='Train $\\mu_i$')\n",
        "ax.plot(time_test, y_predict_mean, color='tab:red', label='Prediction')\n",
        "ax.fill_between(time_test, y_predict_lb, y_predict_ub, color='tab:red',\n",
        "                alpha=0.2)\n",
        "\n",
        "ax.axvline(x=x[index_of_2010], ls='--', color='k')\n",
        "ax.set_xlabel('Time (years)')\n",
        "ax.set_ylabel(r'$CO_2$ (ppm))')\n",
        "ax.legend()\n",
        "ax.set_xlim([np.min(x), np.max(x)]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyKqtnje25gr"
      },
      "source": [
        "7. Make a figure with two overlapping histograms; one showing the MAPE scores for the predictions using the linear model, and one showing the MAPE scores for the predictions using the new model that combines the linear and seasonal trends. Do you see, as expected, a reduction in MAPE?\n",
        "\n",
        "**Note**: It would be nice to add a formal Bayesian model comparison between the two models, and test whether one is better than the other. In the interest of focusing on timeseries we omit that now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-Bh7Kns-Kb"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYSV3vR00QQy"
      },
      "outputs": [],
      "source": [
        "def mape(y_pred_):\n",
        "    return np.mean(np.abs(y_pred_ - y_test) / y_test)\n",
        "\n",
        "#\n",
        "\n",
        "y_predict_samples_seasonal = ...\n",
        "y_predict_samples_linear = ...\n",
        "\n",
        "mape_scores_seasonal = np.apply_along_axis(mape, 1,\n",
        "                                           y_predict_samples_seasonal.T)\n",
        "mape_scores_linear = np.apply_along_axis(mape, 1,\n",
        "                                         y_predict_samples_linear.T)\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(mape_scores_linear, density=True, bins=50, alpha=0.7,\n",
        "         label='Linear')\n",
        "plt.hist(mape_scores_seasonal, density=True, bins=50, alpha=0.7,\n",
        "         label='Linear + seasonal')\n",
        "plt.xlabel('MAPE')\n",
        "plt.ylabel('p(MAPE)')\n",
        "plt.title('Posterior predictive performance')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrsxayWuKr2P"
      },
      "source": [
        "The parameter $\\sigma$ in $y_i \\mid \\mu_i, \\sigma \\sim \\mathcal{N}(\\mu_i, \\sigma)$ indicates how much the actual observations vary around the predicted mean.\n",
        "\n",
        "8. Make a figure with two overlapping histograms again, this time showing the posterior distribution of $\\sigma$ according to either the linear, or the linear+seasonal trends. Why is $\\sigma$ lower for the linear+seasonal model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7aunZc9MSfT"
      },
      "source": [
        "_ANSWER:_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNE8DIfyLjye"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.hist(..., density=True, bins=50, alpha=0.7,\n",
        "         label='Linear')\n",
        "plt.hist(..., density=True, bins=50, alpha=0.7,\n",
        "         label='Linear + seasonal')\n",
        "plt.xlabel(r'$\\sigma$')\n",
        "plt.ylabel(r'$p(\\sigma\\mid x}, y)$')\n",
        "plt.title(r'Posterior distribution of observation noise $\\sigma$')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNfzmIU4Ue_U"
      },
      "source": [
        "## The linear + seasonal + quadratic model\n",
        "\n",
        "Both the MAPE and the posterior distribution of $\\sigma$ (and of course the visualization of the predictions themselves) confirm that the seasonal trend is a welcome addition when trying to model the CO2 concentrations in the air around Mauna Loa. However, there's definitely room for improvement still.\n",
        "\n",
        "By looking at the model fits and predictions, we see that the linear trend works well in the middle of the training range, but is incorrect near the edges (and therefore also near the range we are interested in most; after 2010!). A higher-order trend (like **quadratic**) might be helpful here:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\mu_i \\mid \\beta_0, \\beta_1, \\beta_2, \\beta_3, \\phi x_i &= \\beta_0 + \\beta_1 x_i + \\beta_2 \\sin(2\\pi / 12 x_i + \\phi)  + \\beta_3 x_i^2&& i=1,\\ldots,n\\\\\n",
        "    y_i \\mid \\mu_i, \\sigma & \\sim \\mathcal{N}(\\mu_i, \\sigma)  && i=1,\\ldots,n\\enspace.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "9. Copy your previous JAGS code to the cell below and add the quadratic term. Repeat the steps from before: plot the predictions, plot the MAPE (for all three models in a single figure) and plot the posterior of $\\sigma$ (also for all three models in a single figure)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpwBV7NvtBD6"
      },
      "source": [
        "_ANSWER:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKCPnlYfNMK-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "code = '''\n",
        "model{\n",
        "    ...\n",
        "\n",
        "    for (i in 1:n_train) {\n",
        "        ...\n",
        "    }\n",
        "\n",
        "    for (j in 1:n_test) {\n",
        "        ...\n",
        "    }\n",
        "\n",
        "}\n",
        "'''\n",
        "\n",
        "data = dict(x_train=x_train,\n",
        "            y_train=y_train,\n",
        "            x_test=x_test,\n",
        "            n_train=n_train,\n",
        "            n_test=n_test)\n",
        "num_samples = 10_000\n",
        "num_chains = 4\n",
        "\n",
        "m_quad = pj.Model(code=code, data=data, chains=num_chains)\n",
        "samples_quad = m_quad.sample(num_samples, vars=[...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2vwQAHXNdNv"
      },
      "outputs": [],
      "source": [
        "y_predict_mean = ...\n",
        "y_predict_lb = ...\n",
        "y_predict_ub = ...\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = plt.gca()\n",
        "ax.plot(time_train, y_train, color=colors[0], label='Train data')\n",
        "ax.plot(time_test, y_test, color=colors[1], label='Test data')\n",
        "ax.plot(time_train, ..., color='tab:blue',\n",
        "        label='Train $\\mu_i$')\n",
        "ax.plot(time_test, y_predict_mean, color='tab:red', label='Prediction')\n",
        "ax.fill_between(time_test, y_predict_lb, y_predict_ub, color='tab:red',\n",
        "                alpha=0.2)\n",
        "\n",
        "ax.axvline(x=x[index_of_2010], ls='--', color='k')\n",
        "ax.set_xlabel('Time (years)')\n",
        "ax.set_ylabel(r'$CO_2$ (ppm))')\n",
        "ax.legend()\n",
        "ax.set_xlim([np.min(x), np.max(x)])\n",
        "ax.set_title('Linear + seasonal + quadratic trends');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A774WkZ3OCjt"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.hist(..., density=True, bins=50, alpha=0.7,\n",
        "         label='Linear')\n",
        "plt.hist(..., density=True, bins=50, alpha=0.7,\n",
        "         label='Linear + seasonal')\n",
        "plt.hist(..., density=True, bins=50, alpha=0.7,\n",
        "         label='Linear + seasonal + quadratic')\n",
        "plt.xlabel(r'$\\sigma$')\n",
        "plt.ylabel(r'$p(\\sigma\\mid x}, y)$')\n",
        "plt.title(r'Posterior distribution of observation noise $\\sigma$')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF_NwHcyOKZ7"
      },
      "outputs": [],
      "source": [
        "def mape(y_pred_):\n",
        "    return np.mean(np.abs(y_pred_ - y_test) / y_test)\n",
        "\n",
        "#\n",
        "\n",
        "y_predict_samples_quad = ...\n",
        "\n",
        "y_predict_samples_seasonal = ...\n",
        "\n",
        "y_predict_samples_linear = ...\n",
        "\n",
        "mape_scores_quad = np.apply_along_axis(mape, 1,\n",
        "                                           y_predict_samples_quad.T)\n",
        "mape_scores_seasonal = np.apply_along_axis(mape, 1,\n",
        "                                           y_predict_samples_seasonal.T)\n",
        "mape_scores_linear = np.apply_along_axis(mape, 1,\n",
        "                                         y_predict_samples_linear.T)\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(mape_scores_linear, density=True, bins=50, alpha=0.7,\n",
        "         label='Linear')\n",
        "plt.hist(mape_scores_seasonal, density=True, bins=50, alpha=0.7,\n",
        "         label='Linear + seasonal')\n",
        "plt.hist(mape_scores_quad, density=True, bins=50, alpha=0.7,\n",
        "         label='Linear + seasonal + quad')\n",
        "plt.xlabel('MAPE')\n",
        "plt.ylabel('p(MAPE)')\n",
        "plt.title('Posterior predictive performance')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_uXVCzDXzro"
      },
      "source": [
        "When fitting models to training data, we know that we will always obtain a better fit with a complex model than with a simple one. Our last model is definitely the most complex. For example, the linear model is just a special case of it, with $\\beta_2=\\beta_3=0$.\n",
        "\n",
        "We saw in lecture 7 how Bayesian model comparison through Bayes factors / marginal likelihoods has a form of Occam's razor built-in, that adjusts for model complexity. However, here we computed the predictive performance of the models instead (using the MAPE).\n",
        "\n",
        "10. Argue why we did or did not overfit in this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO87DFOxc4by"
      },
      "source": [
        "_ANSWER:_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 3. Predicting diabetes\n",
        "\n",
        "For the final exercise of this assignment we'll be looking into a logistic regression example (see Lecture 06). This is essentially Bayesian classification.\n",
        "\n",
        "The dataset from this study comes from a Kaggle competition and was collected by the National Institute of Diabetes and Digestive and Kidney Diseases, and contains measurements for $n=768$ women of Pima Indian Heritage. The measurements consist of diagnostic information such as glucose levels, BMI, number of pregnancies, and so forth. Our job is to predict the outcome variable 'diabetes' (0 for no, 1 for yes) based on these predictors, and to figure out which variables are most important for this diagnosis.\n",
        "\n",
        "The code block below loads the data and does some minor preprocessing so the data are 'nice' to work with.\n",
        "\n",
        "Note: This is a relatively large dataset, which means that a JAGS model might run for quite a while. For debugging, you can just use the first $m \\ll n$ records of course!"
      ],
      "metadata": {
        "id": "_qT3N4q2vOtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zscale(x):\n",
        "    return (x - np.mean(x)) / np.std(x)\n",
        "\n",
        "#\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('diabetes.csv', sep=',', header=0)\n",
        "\n",
        "# We have some missing values; glucose, blood pressure, skin thickness, insulin, BMI, DiabetesPedigreeFunction contain zeros which is impossible\n",
        "# We fill in these missing values by *imputing* the median\n",
        "\n",
        "for col in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction']:\n",
        "    df[col] = df[col].replace(0, df[col].median())\n",
        "\n",
        "# Since the predictors are measured in different units, we rescale them so we can interpret the corresponding coefficients later on\n",
        "\n",
        "for col in ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction']:\n",
        "    df[col] = zscale(df[col])\n",
        "\n",
        "df['Age'] = df['Age'] / 100\n",
        "\n",
        "# The predictors in numpy format:\n",
        "X = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']].to_numpy()\n",
        "\n",
        "# And the outcome that we will try to predict\n",
        "y = df['Outcome']\n",
        "\n",
        "n, p = X.shape\n",
        "\n",
        "# split train/test\n",
        "\n",
        "train_fraction = 0.8\n",
        "n_train = int(n*train_fraction)\n",
        "n_test = n - n_train\n",
        "\n",
        "X_train = X[0:n_train, :]\n",
        "y_train = y[0:n_train]\n",
        "\n",
        "X_test = X[n_train:, :]\n",
        "y_test = y[n_train:]"
      ],
      "metadata": {
        "id": "3khrMnBQvU4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above also splits the data into a set for training and a set for testing; that will tell us how well our model generalizes to new observations.\n",
        "\n",
        "1. Implement a logistic regression model in JAGS and train it on the training data `X_train` and `y_train`. Don't forget to include an intercept term $\\beta_0$!"
      ],
      "metadata": {
        "id": "pYhbBDhwvaaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = '''\n",
        "model {\n",
        "    ...\n",
        "}\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "data = dict(...)\n",
        "\n",
        "# How long we sample\n",
        "num_iter = 20_000\n",
        "# How many distinct sampling runs\n",
        "num_chains = 4\n",
        "\n",
        "model = pj.Model(model, data=data, chains=num_chains)\n",
        "samples = model.sample(num_iter, vars=[...])"
      ],
      "metadata": {
        "id": "YdJFzMUwvckO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Make a figure showing for all predictors a histogram of the distribution and the 95% HDI interval indicated as a shaded area (for an example, see slide 45 in Lecture 06). Using the standard approach from the course, which predictors are important in predicting whether someone will have diabetes?"
      ],
      "metadata": {
        "id": "vkyuqM1PvinB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-I5GQzTUvlqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how well our model can predict diabetes for data it has not yet seen.\n",
        "\n",
        "We can do this via the posterior predictive distribution $p(y^* \\mid \\mathbf{y})$, but because this distribution includes all the uncertainty of our parameters, plus the uncertainty of the predictive distribution, it is very wide. If we sample a prediction from such a wide distribution, it will often be wrong, because of this width.\n",
        "\n",
        "If we just want to look at the performance of the logistic regression model as a classifier, it is helpful to summarize the distributions of the regression parameters with their expectation instead. As you recall, you can obtain these by simply taking the mean over your samples.\n",
        "\n",
        "Compute the mean of your model parameters (intercept and coefficients) and using those means, compute for every test individual $i$ (with predictors $\\mathbf{x}_i^*) $the probability\n",
        "$$\n",
        "\\mu_i^*= p(y_i^* =1 \\mid \\mathbf{y}) = \\text{logit}^{-1}\\left( \\bar{\\beta}_0 + \\bar{\\mathbf{\\beta}}(\\mathbf{x}_i^*)^\\top\\right)\n",
        "$$ where the bar symbol $\\bar{\\cdot}$ indicates the posterior mean of those parameters.\n",
        "\n",
        "Now $\\mu_i^*$ is the probability that  $y_i^*=1$, so we can apply a threshold to obtain an actual prediction. Let's say the predictions are $y_i^* = \\mu_i^* > 0.5$.\n",
        "\n",
        "3. Use these predictions as the output of your Bayesian classifier. What is the accuracy of this tool?"
      ],
      "metadata": {
        "id": "5RISqv6AvnSa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BvTrk5GvqPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}